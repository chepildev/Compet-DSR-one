{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Pipeline**\n",
    "\n",
    "| **Steps**                                              | **Script files**                          |\n",
    "|-----------------------------------------------------------|-------------------------------------------|\n",
    "| 1) Read and pre-process data                              | pre_processing.py                         |\n",
    "| 2) Feature engineering                                    | feature_engineering.py                    |\n",
    "| 3) Train models                                           | model_training.py, <br>tree_model_training.py |\n",
    "| 4) Predict on test_features <br>and write submission file | final_predict.py                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import scripts.pre_processing as pp\n",
    "import scripts.feature_engineering as fe\n",
    "import scripts.model_evaluation as me\n",
    "from scripts.model_training import Model\n",
    "from scripts.tree_model_training import rf_model\n",
    "from scripts.model_evaluation import regression_evaluation\n",
    "import scripts.final_predict as fp\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data files \n",
    "train_features = pd.read_csv('./data/dengue_features_train.csv')\n",
    "train_target = pd.read_csv('./data/dengue_labels_train.csv')\n",
    "test_features = pd.read_csv('./data/dengue_features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iq shape: (520, 24)\n",
      "train_sj shape: (928, 24)\n"
     ]
    }
   ],
   "source": [
    "# Merge features and target data\n",
    "data = pp.merge_data(train_features, train_target, test_features, inc_test=False)\n",
    "\n",
    "# Run processing and split by city\n",
    "train_iq = pp.pre_process(data, 'iq', remove_anomalies=True)\n",
    "train_sj = pp.pre_process(data, 'sj', remove_anomalies=True)\n",
    "\n",
    "# Run checks for missing values\n",
    "assert train_iq.isnull().any().any() == False\n",
    "assert train_sj.isnull().any().any() == False\n",
    "print(f'train_iq shape: {train_iq.shape}') \n",
    "print(f'train_sj shape: {train_sj.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.boxplot(train_sj['total_cases'])\n",
    "d = train_iq.loc[train_iq['total_cases'] <80, :] \n",
    "plt.boxplot(d['total_cases'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature engineering \n",
    "train_iq = fe.feature_engineer_1(train_iq)\n",
    "train_sj = fe.feature_engineer_1(train_sj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and cross-validation sets\n",
    "X_train_sj, y_train_sj, X_test_sj, y_test_sj = pp.train_cv_split(train_sj, city='sj')\n",
    "X_train_iq, y_train_iq, X_test_iq, y_test_iq = pp.train_cv_split(train_iq, city='iq')\n",
    "\n",
    "# Check compatible sizes for models:\n",
    "assert len(X_train_sj) == len(y_train_sj)\n",
    "assert len(X_test_sj) == len(y_test_sj)\n",
    "assert len(X_train_iq) == len(y_train_iq)\n",
    "assert len(X_test_iq) == len(y_test_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 23) (130, 23)\n",
      "(260, 23) (130, 23)\n",
      "(390, 23) (130, 23)\n"
     ]
    }
   ],
   "source": [
    "# Train test split with sklearn \n",
    "tss = TimeSeriesSplit(n_splits = 3)\n",
    "\n",
    "X_iq = train_iq.drop(labels=['total_cases'], axis=1)\n",
    "y_iq = train_iq['total_cases']\n",
    "X_sj = train_sj.drop(labels=['total_cases'], axis=1)\n",
    "y_sj = train_sj['total_cases']\n",
    "\n",
    "for train_index, test_index in tss.split(X_iq):\n",
    "    X_train, X_test = X_iq.iloc[train_index, :], X_iq.iloc[test_index,:]\n",
    "    y_train, y_test = y_iq.iloc[train_index], y_iq.iloc[test_index]\n",
    "    print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model predictions for San Jose (sj)\n",
    "bl_pred_train = np.tile(np.mean(y_train_sj), len(y_train_sj))\n",
    "bl_pred_test = np.tile(np.mean(y_test_sj), len(y_test_sj))\n",
    "regression_evaluation(y_train_sj, y_test_sj, bl_pred_train, bl_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model predictions for Iquitos (Iq)\n",
    "bl_pred_train = np.tile(np.mean(y_train_iq), len(y_train_iq))\n",
    "bl_pred_test = np.tile(np.mean(y_test_iq), len(y_test_iq))\n",
    "regression_evaluation(y_train_iq, y_test_iq, bl_pred_train, bl_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree model for IQ\n",
    "rf_model(X_train_iq, y_train_iq, X_test_iq, y_test_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree model for SJ \n",
    "rf_model(X_train_sj, y_train_sj, X_test_sj, y_test_sj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# LTSM MODEL \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler \n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Bidirectional, Dropout, Activation, Dense, LSTM\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m CuDNNLSTM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# LTSM MODEL \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import tensorflow as tf\n",
    "from tf.keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM\n",
    "from tf.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tf.keras.models import Sequential\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X = scaler.fit_transform(X_iq)\n",
    "\n",
    "model = Sequential(name=\"LSTM-Model\")\n",
    "model.add(LSTM(units=3, input_shape=(2,10), return_sequences=False))\n",
    "model.add(Dense(1, activation='Linear'))\n",
    "model.complile(loss='mae', optimizer='adam')\n",
    "model.fit(scaled_X, y_iq, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTSM MODEL \n",
    "import sklearn.preprocessing import MinMaxScaler \n",
    "import tensorflow as tf\n",
    "from tf.keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM\n",
    "from tf.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tf.keras.models import Sequential\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X_iq)\n",
    "\n",
    "model = Sequential(name=\"LSTM-Model\")\n",
    "model.add(LSTM(units=3, input_shape=(2,10), return_sequences=False)\n",
    "model.add(Dense(1, activation='Linear'))\n",
    "model.complile(loss='mae', optimizer='adam')\n",
    "model.fit(X_iq, y_iq, epochs=10)\n",
    "\n",
    "model = Sequential(name=\"LSTM-Model\") # Model\n",
    "model.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\n",
    "model.add(Bidirectional(LSTM(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False), name='Hidden-LSTM-Encoder-Layer')) # Encoder Layer\n",
    "model.add(RepeatVector(Y_train.shape[1], name='Repeat-Vector-Layer')) # Repeat Vector\n",
    "model.add(Bidirectional(LSTM(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-LSTM-Decoder-Layer')) # Decoder Layer\n",
    "model.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer')) # Output Layer, Linear(x) = x\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test data with chosen model and write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run pipeline on dataset including test_features, and then take only test_features to run the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data and chosen model and hyperparameters for final prediction\n",
    "\n",
    "# Iquitos, iq\n",
    "final_test_iq = train_iq.drop(['total_cases'], axis=1)\n",
    "X_train_iq = X_train_iq\n",
    "y_train_iq = y_train_iq\n",
    "model_iq = 'RandomForestRegressor'\n",
    "params_iq = {}\n",
    "\n",
    "# San Jose, sj\n",
    "final_test_sj = train_sj.drop(['total_cases'], axis=1)\n",
    "X_train_sj = X_train_sj\n",
    "y_train_sj = y_train_sj\n",
    "model_sj = 'RandomForestRegressor'\n",
    "params_sj = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final predictions and reformat for submission\n",
    "final_iq = fp.final_predict(final_test_iq, X_train_iq, y_train_iq, \n",
    "              city='iq', model=model_iq, params=params_iq)\n",
    "final_sj = fp.final_predict(final_test_sj, X_train_sj, y_train_sj, \n",
    "              city='sj', model=model_sj, params=params_sj)\n",
    "\n",
    "# Merge the two cities into one DataFrame and write to new csv file \n",
    "fp.write_submission(final_iq, final_sj) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSR-comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
